{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b987337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as dom\n",
    "import math\n",
    "import operator\n",
    "import logging\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numLinhas(arquivos):\n",
    "    '''\n",
    "    Verifica a quantidade de linhas de um arquivo.\n",
    "    '''    \n",
    "    contador_linhas = 0 \n",
    "    try:\n",
    "        for arquivo in arquivos:\n",
    "            current_directory = os.getcwd()\n",
    "            parent_directory  = os.path.dirname(current_directory)\n",
    "            path = os.path.join(parent_directory,'data',arquivo)\n",
    "\n",
    "            with open(path, 'r') as arquivo:\n",
    "                for linha in arquivo:\n",
    "                    contador_linhas += 1\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            for arquivo in arquivos:\n",
    "                current_directory = os.getcwd()\n",
    "                parent_directory  = os.path.dirname(current_directory)\n",
    "                path = os.path.join(parent_directory,'data',arquivo)\n",
    "                \n",
    "                with open(path, 'r') as arquivo:\n",
    "                    for linha in arquivo:\n",
    "                        contador_linhas += 1\n",
    "        except:\n",
    "            for arquivo in arquivos:\n",
    "                current_directory = os.getcwd()\n",
    "                parent_directory  = os.path.dirname(current_directory)\n",
    "                path = os.path.join(parent_directory,'result',arquivo)\n",
    "                \n",
    "                with open(path, 'r') as arquivo:\n",
    "                    for linha in arquivo:\n",
    "                        contador_linhas += 1\n",
    "                    \n",
    "    return contador_linhas\n",
    "    \n",
    "\n",
    "def ler_tags(tags, arquivo):\n",
    "    '''\n",
    "    Lê as ocorrências das Tags em um arquivo XML.\n",
    "    '''        \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'data',arquivo)\n",
    "    \n",
    "   # Leitura do arquivo XML\n",
    "    doc = dom.parse(path)\n",
    "\n",
    "    # Creiacao da Matrix onde as colunas são as tags e as linhas são os registros da tag pai\n",
    "    result = []\n",
    "    for tag in tags:\n",
    "        result.append([elemento.firstChild.data for elemento in doc.getElementsByTagName(tag)])\n",
    "    return [tags] + np.matrix(result).T.tolist()\n",
    "    \n",
    "    \n",
    "def ler_tags_case(tags, arquivos):\n",
    "    '''\n",
    "    Lê as ocorrências das Tag em um arquivo XML e caso uma das tag for ABSTRACT e a entidade corrente não tiver, lerá a EXTRACT.\n",
    "    ''' \n",
    "    result_final = []\n",
    "    for arquivo in arquivos:\n",
    "        current_directory = os.getcwd()\n",
    "        parent_directory  = os.path.dirname(current_directory)\n",
    "        path = os.path.join(parent_directory,'data',arquivo)\n",
    "\n",
    "        # Leitura do arquivo XML\n",
    "        doc  = dom.parse(path)\n",
    "        tree = ET.parse(path)  \n",
    "        root = tree.getroot()\n",
    "\n",
    "        result = []    \n",
    "        for tag in tags:\n",
    "            \n",
    "            if tag == 'ABSTRACT': \n",
    "                \n",
    "                values = []\n",
    "                for record in root.findall('RECORD'):\n",
    "                    primeira_tag = record.find('ABSTRACT')\n",
    "                    segunda_tag = record.find('EXTRACT')\n",
    "                    \n",
    "                    if primeira_tag is not None:\n",
    "                        primeira_tag_valor = primeira_tag.text\n",
    "                        values.append(primeira_tag_valor)\n",
    "                        \n",
    "                    elif segunda_tag is not None:\n",
    "                        segunda_tag_valor = segunda_tag.text\n",
    "                        values.append(segunda_tag_valor)\n",
    "                    else:\n",
    "                        values.append('')\n",
    "                        \n",
    "                result.append(values)\n",
    "                \n",
    "            else:\n",
    "                result.append([elemento.firstChild.data.replace(' ','') for elemento in doc.getElementsByTagName(tag)])\n",
    "                \n",
    "        result_final = result_final + list([list(row) for row in zip(*result)])\n",
    "\n",
    "    return [tags] + result_final\n",
    "\n",
    "\n",
    "def ler_tag_attr(tag, attr, arquivo):\n",
    "    '''\n",
    "    Lê o atributo de uma tag em um arquivo XML.\n",
    "    '''  \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'data',arquivo)\n",
    "    \n",
    "    # Leitura do arquivo XML\n",
    "    doc = dom.parse(path)\n",
    "\n",
    "    # Criacao da Matrix onde as colunas são as tags e as linhas são os registros da tag pai\n",
    "    result = []\n",
    "    elementos = doc.getElementsByTagName(tag)\n",
    "    for element in elementos:\n",
    "        result.append([element.firstChild.data, element.getAttribute(attr)])\n",
    "    \n",
    "    return [[tag, attr]] + result    \n",
    "\n",
    "\n",
    "def CriaConsulta(texto):\n",
    "    '''\n",
    "    Cria uma consulta baseado nas palavras chaves do texto.\n",
    "    '''    \n",
    "    texto_sem_acentos = unidecode(texto)                                                             \n",
    "    texto_maiusculo = texto_sem_acentos.upper()                                                      \n",
    "    texto_sem_quebra_e_aspas = texto_maiusculo.replace('\\n',' ').replace('\"','').replace('-',' ')    \n",
    "    \n",
    "    tokens = word_tokenize(texto_sem_quebra_e_aspas)\n",
    "    pontuacoes = set(string.punctuation)\n",
    "    \n",
    "    palavras = [token for token in tokens if token not in pontuacoes and token.lower() not in stopwords.words('english')] \n",
    "    texto = ' '.join(palavras)\n",
    "    \n",
    "    return texto\n",
    "    \n",
    "    \n",
    "def processa_arq_consulta(tags_originais):\n",
    "    '''\n",
    "    Gera a lista que será gravada no arquivo citado na instrução CONSULTA do arquivo PC.CFG.\n",
    "    '''\n",
    "    tags_tratadas = [[tags_originais[0][0], tags_originais[0][1]]] + [[i[0],CriaConsulta(i[1])] for i in tags_originais[1:]]\n",
    "    \n",
    "    return tags_tratadas\n",
    "        \n",
    "    \n",
    "def escreve_arq_consulta(tags_tratadas, nome_arq_saida):\n",
    "    '''\n",
    "    A partir do resultadoda processa_arq_consulta, escreve o arquivo que é o citado na instrução CONSULTA do arquivo PC.CFG.\n",
    "    '''    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_saida)\n",
    "    \n",
    "    with open(path, 'w', newline='') as arquivo_csv:\n",
    "        writer = csv.writer(arquivo_csv, delimiter=';')\n",
    "        writer.writerows(tags_tratadas)\n",
    "        \n",
    "\n",
    "def processa_arq_esperados(resultado1, resultado2):\n",
    "    '''\n",
    "    Gera a lista contendo o numero de votos de cada documento recuperado pela consulta, o numero do documento e o numero \n",
    "    da consulta. Esta lista será gravada no arquivo citado na instrução ESPERADOS do arquivo PC.CFG.\n",
    "    '''\n",
    "    header = ['QueryNumber', 'DocNumber', 'DocVotes']\n",
    "    result1 = resultado1[1:]\n",
    "    result2 = resultado2[1:]\n",
    "    \n",
    "    avalIndex = 0\n",
    "    result = []\n",
    "    for i in result1:\n",
    "        \n",
    "        queryNum = i[0]\n",
    "        avaliacoes = int(i[1])\n",
    "        for j in range(avaliacoes):\n",
    "            \n",
    "            document = result2[avalIndex][0]\n",
    "            aval = np.sum([1 for k in result2[avalIndex][1] if int(k)>0])\n",
    "            \n",
    "            linha = [queryNum, str(document).zfill(5), aval] \n",
    "            result.append(linha)\n",
    "            avalIndex = avalIndex + 1\n",
    "    \n",
    "    return [header] + result\n",
    "\n",
    "\n",
    "def escreve_arq_esperados(arq_esperados, nome_arq_saida):\n",
    "    '''\n",
    "    A partir do resultado da processa_arq_esperados, escreve o arquivo que é o citado na instrução ESPERADOS do arquivo PC.CFG.\n",
    "    '''        \n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_saida)\n",
    "    \n",
    "    with open(path, 'w', newline='') as arquivo_csv:\n",
    "        writer = csv.writer(arquivo_csv, delimiter=';')\n",
    "        writer.writerows(arq_esperados)\n",
    "\n",
    "        \n",
    "def processa_arq_lista_invertida(lista):    \n",
    "    '''\n",
    "    Cria uma lista_invertida para as palavras encontradas nos ABSTRAC/ EXTRACT dos documentos contidos nos arquivos de entrada.\n",
    "    '''    \n",
    "    lista_invertida = {}\n",
    "    for linha in lista[1:]:\n",
    "        \n",
    "        numero_documento = linha[0]\n",
    "        texto = linha[1]\n",
    "        \n",
    "        texto_sem_acentos = unidecode(texto)                                                            \n",
    "        texto_maiusculo = texto_sem_acentos.upper()                                                     \n",
    "        texto_sem_quebra_e_aspas = texto_maiusculo.replace('\\n',' ').replace('\"','').replace('-',' ')\n",
    "\n",
    "        tokens = word_tokenize(texto_sem_quebra_e_aspas)\n",
    "        pontuacoes = set(string.punctuation)\n",
    "    \n",
    "        palavras = [token for token in tokens if token not in pontuacoes and token.lower() not in stopwords.words('english')]\n",
    "        for palavra in palavras:\n",
    "            \n",
    "            if palavra not in lista_invertida:\n",
    "                lista_invertida[palavra] = []\n",
    "                \n",
    "            lista_invertida[palavra].append(numero_documento)\n",
    "    \n",
    "    return lista_invertida\n",
    "\n",
    "\n",
    "def escreve_arq_lista_invertida(lista_invertida, nome_arq_saida):\n",
    "    '''\n",
    "    Escreve a lista invertida no qual guarda os documentos em que cada termo aparece (mantendo repetições) em um arquivo cujo \n",
    "    nome é citado na instrução ESCREVA do arquivo GLI.CFG.\n",
    "    '''        \n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_saida)\n",
    "    \n",
    "    with open(path, 'w', newline='') as file:\n",
    "\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow(['PALAVRA', 'DOCUMENTOS'])\n",
    "        for palavra, documentos in lista_invertida.items():\n",
    "            \n",
    "            writer.writerow([palavra, documentos])\n",
    "\n",
    "\n",
    "def processa_arq_index(nome_arq_listaI, norm=0):\n",
    "    '''\n",
    "    Cria uma matrix termo-documento na forma de um dicionário onde para cada documento temos os termos que aparecem (e não \n",
    "    aparecem) neles e o TF-IDF que qualifica a relevância do termo no respectivo documento. Vale ressaltar que como resolvi \n",
    "    guardar a informação da base da matriz para facilitar cálculos futuros, para cada documento, então, foi adicionado também \n",
    "    uma entrada para os termos que não aparecem nele mas está presente em algum dos demais. Além disso, foi dado o valor 0\n",
    "    para seu TF-IDF.\n",
    "    '''        \n",
    "    dic_freq_docs = {}\n",
    "    dic_freq_words = {}\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_listaI)\n",
    "\n",
    "    with open(path, 'r') as arquivo:\n",
    "        linhas = arquivo.readlines()[1:]\n",
    "\n",
    "        for linha in linhas:\n",
    "            word, docs = linha.strip().split(';')\n",
    "            doc_list = [doc.strip().replace('[','').replace(']','').replace(\"'\", \"\") for doc in docs.split(',')]  # Converte a string em uma \n",
    "                                                                                                                  # lista de documentos\n",
    "                            \n",
    "            if word.isalpha() and len(word) >= 2:\n",
    "            \n",
    "                for doc in doc_list:\n",
    "                    \n",
    "                    if doc not in dic_freq_docs:\n",
    "                        dic_freq_docs[doc] = {}\n",
    "                    \n",
    "                    if word not in dic_freq_docs[doc]:\n",
    "                        dic_freq_docs[doc][word] = 0\n",
    "                        \n",
    "                    dic_freq_docs[doc][word] += 1\n",
    "                \n",
    "                if word not in dic_freq_words:\n",
    "                    dic_freq_words[word] = 0\n",
    "                    \n",
    "                dic_freq_words[word] += len(set(doc_list)) # Adiciona no dicionário o total de documentos em que a palavra aparece\n",
    "                \n",
    "    # Cálculo do IDF\n",
    "    total_docs = len(dic_freq_docs)\n",
    "    dic_idf = {word: math.log(total_docs / freq) for word, freq in dic_freq_words.items()}\n",
    "\n",
    "    # Cálculo do TF-IDF normalizado\n",
    "    index_tf_idf = {}\n",
    "    for doc, word_freqs in dic_freq_docs.items():\n",
    "        index_tf_idf[doc] = {}\n",
    "        \n",
    "        for word, freq in dic_freq_words.items():\n",
    "            \n",
    "            if word in word_freqs:\n",
    "                index_tf_idf[doc][word] = (1 + math.log(freq)) * dic_idf[word] if norm != 0 else 1 * dic_idf[word]\n",
    "                \n",
    "    return index_tf_idf\n",
    "\n",
    "\n",
    "def escreve_arq_index(index, nome_arq_saida):\n",
    "    '''\n",
    "    A partir do resultado da processa_arq_index, escreve o arquivo que é o citado na instrução ESPERADOS do arquivo INDEX.CFG.\n",
    "    ''' \n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_saida)\n",
    "    \n",
    "    with open(path, 'w', newline='') as arquivo:\n",
    "        writer = csv.writer(arquivo, delimiter=';')\n",
    "        writer.writerow([\"Documento\", \"Palavra\", \"TF-IDF\"])\n",
    "\n",
    "        for doc, word_freqs in index.items():\n",
    "            for word, tf_idf in word_freqs.items():\n",
    "                writer.writerow([doc, word, tf_idf])\n",
    "                \n",
    "                \n",
    "def carrega_modelo_vetorial(nome_arq_modelo):\n",
    "    '''\n",
    "    Lê o arquivo que contém o modelo vetorial e transforma em um dicionário contendo os documentos e os pesos das palavras da\n",
    "    base matricial na sua respectiva representação vetorial (tf-idf ou 0).\n",
    "    '''\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_modelo)\n",
    "    \n",
    "    index_tf_idf = {}\n",
    "    with open(path, 'r') as arquivo:\n",
    "        reader = csv.reader(arquivo, delimiter=';')\n",
    "        next(reader)  \n",
    "\n",
    "        for linha in reader:\n",
    "            doc, word, tf_idf = linha\n",
    "            if doc not in index_tf_idf:\n",
    "                index_tf_idf[doc] = {}\n",
    "            index_tf_idf[doc][word] = float(tf_idf)\n",
    "\n",
    "    return index_tf_idf\n",
    "\n",
    "\n",
    "def calcular_vetor_consulta(consulta, vetor_doc):\n",
    "    '''\n",
    "    A consulta aqui é uma das consultas do arquivo que contem as consultas, e a partir dela geramos um vetor de comprimento \n",
    "    igual à base de palavras do nosso modelo vetorial com o auxílio de um dos vetores documentos presente no modelo. Atrubuímos\n",
    "    o peso 1 para as palavras em comum e 0 para as palavras da base que não pertencem à consulta. Retorna o vetor gerado.\n",
    "    '''\n",
    "    palavras = list(vetor_doc.keys())  \n",
    "    total_palavras = len(palavras)\n",
    "    \n",
    "    vetor_consulta = np.zeros(total_palavras)\n",
    "    for palavra, freq in consulta.items():\n",
    "        \n",
    "        if palavra in palavras:\n",
    "            indice = palavras.index(palavra)\n",
    "            vetor_consulta[indice] = 1\n",
    "            \n",
    "    return vetor_consulta\n",
    "\n",
    "\n",
    "\n",
    "def calcular_similaridade(vetor_consulta, vetor_doc):\n",
    "    '''\n",
    "    Calacula a similaridade entre o vetor_consulta e o vetor_doc utilizando o cosseno entre eles. \n",
    "    '''    \n",
    "    vetor_consulta = np.array(vetor_consulta)\n",
    "    vetor_doc = np.array(list(vetor_doc.values()))\n",
    "    \n",
    "    if np.linalg.norm(vetor_consulta) == 0 or np.linalg.norm(vetor_doc) == 0:\n",
    "        similaridade = 0\n",
    "    else:\n",
    "        similaridade = np.dot(vetor_consulta, vetor_doc) / (np.linalg.norm(vetor_consulta) * np.linalg.norm(vetor_doc))\n",
    "    \n",
    "    return similaridade\n",
    "\n",
    "\n",
    "def processa_arq_busca(nome_arq_modelo, nome_arq_consultas):\n",
    "    '''\n",
    "    Realiza a busca dos documentos mais relevantes para cada consulta de acordo com a similaridade entre eles. O retorno dessa\n",
    "    funcao é uma lista contendo as consultas e uma lista de tuplas que contem a posição do ranking de similaridade, os documentos \n",
    "    e os valores de similaridade, isso para cada consulta. O nome do arquivo de consultas se encontra na sessão CONSULTAS do \n",
    "    arquivo BUSCA.CGF e o nome do arquivo modelo se encontra na sessão MODELO do mesmo arquivo.\n",
    "    '''\n",
    "    modelo = carrega_modelo_vetorial(nome_arq_modelo)\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arq_consultas)\n",
    "    \n",
    "    consultas = {}\n",
    "    with open(path, 'r') as arquivo:\n",
    "        reader = csv.reader(arquivo, delimiter=';')\n",
    "        next(reader)  \n",
    "        \n",
    "        for linha in reader:\n",
    "            numquery, query = linha\n",
    "            consultas[numquery] = {}\n",
    "\n",
    "            # Processar a consulta para extrair as palavras e ocorrências\n",
    "            palavras = query.split()  # Separa as palavras da consulta\n",
    "            for palavra in palavras:\n",
    "                \n",
    "                if palavra.isalpha() and len(palavra) >= 2:\n",
    "                    \n",
    "                    if palavra not in consultas[numquery]:\n",
    "                        consultas[numquery][palavra] = 1\n",
    "                \n",
    "    resultados = []\n",
    "    for consulta_id, consulta in consultas.items():\n",
    "                \n",
    "        # Calcular a similaridade entre a consulta e os documentos\n",
    "        similaridade = {}\n",
    "        for doc, vetor_doc in modelo.items():\n",
    "            vetor_consulta = calcular_vetor_consulta(consulta, vetor_doc)\n",
    "            similaridade[doc] = calcular_similaridade(vetor_consulta, vetor_doc)\n",
    "\n",
    "        # Ordenar os documentos por similaridade\n",
    "        documentos_ordenados = sorted(similaridade.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        # Criar a lista de ternos ordenados\n",
    "        lista_ternos = [(posicao, doc, distancia) for posicao, (doc, distancia) in enumerate(documentos_ordenados, start=1)]\n",
    "\n",
    "        # Adicionar o resultado à lista de resultados\n",
    "        resultados.append((consulta_id, lista_ternos))\n",
    "    \n",
    "    return resultados\n",
    "    \n",
    "    \n",
    "def escreve_arq_busca(busca, nome_arquivo):\n",
    "    '''\n",
    "    Escreve o resultado da função processa_arq_busca em um arquivo cujo nome se encontra na sessão RESULTADOS do arquivo \n",
    "    BUSCA.CFG.\n",
    "    '''\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    path = os.path.join(parent_directory,'result',nome_arquivo)\n",
    "    \n",
    "    with open(path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerow(['Consulta', 'Resultado'])\n",
    "\n",
    "        for query_id, query_results in busca:\n",
    "            ranked_results = sorted(query_results, key=lambda x: x[0])\n",
    "            result_str = str([(rank, doc_id, distance) for rank, doc_id, distance in ranked_results])\n",
    "            writer.writerow([query_id, result_str.replace('\"','').replace(\"'\",'')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fe874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o formato do log\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Criar os manipuladores de console e arquivo e atribuir o Formatter\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "path = os.path.join(parent_directory, 'log.txt')\n",
    "\n",
    "file_handler = logging.FileHandler(path, 'w')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Configurar o logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Adicionar os manipuladores ao logger\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Limpando a saída\n",
    "logger.handlers.clear()\n",
    "\n",
    "\n",
    "def ler_dados():\n",
    "    resultado = {}\n",
    "    \n",
    "    # Ler arquivo de configuração\n",
    "    logger.info('Iniciando leitura do arquivos de configuração...')\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    parent_directory  = os.path.dirname(current_directory)\n",
    "    config            = configparser.ConfigParser()\n",
    "    linhas_config     = 0\n",
    "    \n",
    "    arquivo_config              = os.path.join(parent_directory, 'PC.cfg')\n",
    "    config.read(arquivo_config)    \n",
    "    resultado['leia_pc_r']      = config.get('LEIA', 'nome_arquivo')\n",
    "    resultado['concultas_pc_w'] = config.get('CONSULTAS', 'nome_arquivo')\n",
    "    resultado['esperados_pc_w'] = config.get('ESPERADOS', 'nome_arquivo')\n",
    "    linhas_config              += 4\n",
    "\n",
    "    arquivo_config             = os.path.join(parent_directory, 'GLI.cfg')\n",
    "    config.read(arquivo_config) \n",
    "    resultado['leia_gli_r']    = [value for key, value in config.items('LEIA')]\n",
    "    resultado['escreva_gli_w'] = config.get('ESCREVA', 'nome_arquivo')\n",
    "    linhas_config             += 2+len(resultado['leia_gli_r'])\n",
    "    \n",
    "    arquivo_config               = os.path.join(parent_directory, 'INDEX.cfg')\n",
    "    config.read(arquivo_config) \n",
    "    resultado['leia_index_r']    = config.get('LEIA', 'nome_arquivo')\n",
    "    resultado['escreva_index_w'] = config.get('ESCREVA', 'nome_arquivo')\n",
    "    linhas_config               += 3\n",
    "    \n",
    "    arquivo_config                  = os.path.join(parent_directory, 'BUSCA.cfg')\n",
    "    config.read(arquivo_config) \n",
    "    resultado['modelo_busca_r']     = config.get('MODELO', 'nome_arquivo')\n",
    "    resultado['consultas_busca_r']  = config.get('CONSULTAS', 'nome_arquivo')\n",
    "    resultado['resultados_busca_w'] = config.get('RESULTADOS', 'nome_arquivo')\n",
    "    linhas_config                  += 4\n",
    "    \n",
    "    logger.info('Arquivos de configuração lidos.\\n')\n",
    "\n",
    "    # Ler arquivo de dados\n",
    "    logger.info('Iniciando leitura do arquivo de dados...')\n",
    "    \n",
    "    resultado['tags1']    = ['QueryNumber', 'QueryText']\n",
    "    resultado['tags2']    = ['QueryNumber', 'Results']\n",
    "    resultado['tags3']    = ['RECORDNUM', 'ABSTRACT']\n",
    "    resultado['tag_attr'] = 'Item'\n",
    "    resultado['attr']     = 'score'\n",
    "    \n",
    "    linhas_dados = numLinhas([resultado['leia_pc_r']]) + numLinhas(resultado['leia_gli_r'])                 + numLinhas([resultado['leia_index_r']]) + numLinhas([resultado['modelo_busca_r']])                 + numLinhas([resultado['modelo_busca_r']])\n",
    "    \n",
    "    logger.info(f'Arquivo de dados lidos.')\n",
    "    logger.info(f'Total de linhas lidas: {linhas_config + linhas_dados}.\\n\\n\\n')\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "def processar_dados(resultado):\n",
    "    resultado2 = {}\n",
    "    \n",
    "    # Processamento de consultas\n",
    "    logger.info('Iniciando processamento de consultas...')\n",
    "    inicio = time.time()\n",
    "    \n",
    "    result_leia1_pc = ler_tags(resultado['tags1'], resultado['leia_pc_r'])\n",
    "    result_leia2_pc = ler_tags(resultado['tags2'], resultado['leia_pc_r'])\n",
    "    result_leia3_pc = ler_tag_attr(resultado['tag_attr'], resultado['attr'], resultado['leia_pc_r'])\n",
    "    \n",
    "    resultado2['arq_consulta']  = processa_arq_consulta(result_leia1_pc)\n",
    "    resultado2['arq_esperados'] = processa_arq_esperados(result_leia2_pc, result_leia3_pc)\n",
    "    \n",
    "    fim = time.time()\n",
    "    tempo_medio_consultas = fim - inicio\n",
    "    logger.info(f'Consultas Processadas. Tempo médio de processamento de consultas: {tempo_medio_consultas} segundos\\n')\n",
    "\n",
    "    # Processamento de documentos\n",
    "    logger.info('Iniciando processamento dos documentos...')\n",
    "    inicio = time.time()\n",
    "        \n",
    "    result_leia_gli = ler_tags_case(resultado['tags3'], resultado['leia_gli_r'])\n",
    "    resultado2['lista_invertida'] = processa_arq_lista_invertida(result_leia_gli)\n",
    "    \n",
    "    fim = time.time()\n",
    "    tempo_medio_documentos = fim - inicio\n",
    "    logger.info(f'Documentos processados. Tempo médio de processamento de documentos: {tempo_medio_documentos} segundos\\n')\n",
    "\n",
    "    # Processamento de palavras\n",
    "    logger.info('Iniciando processamento de palavras...')\n",
    "    inicio = time.time()\n",
    "    resultado2['result_leia_index'] = processa_arq_index(resultado['leia_index_r'])\n",
    "    resultado2['result_busca'] = processa_arq_busca(resultado['modelo_busca_r'], resultado['consultas_busca_r'])\n",
    "    \n",
    "    fim = time.time()\n",
    "    tempo_medio_palavras = fim - inicio\n",
    "    logger.info(f'Palavras processadas. Tempo médio de processamento de palavras: {tempo_medio_palavras} segundos\\n\\n\\n')\n",
    "    \n",
    "    return resultado2\n",
    "    \n",
    "\n",
    "def salvar_dados(resultado, resultado2):\n",
    "    \n",
    "    logger.info('Iniciando salvamento...')\n",
    "    escreve_arq_consulta(resultado2['arq_consulta'], resultado['concultas_pc_w'])\n",
    "    escreve_arq_esperados(resultado2['arq_esperados'], resultado['esperados_pc_w'])\n",
    "    escreve_arq_lista_invertida(resultado2['lista_invertida'], resultado['escreva_gli_w'])\n",
    "    escreve_arq_index(resultado2['result_leia_index'], resultado['escreva_index_w'])\n",
    "    escreve_arq_busca(resultado2['result_busca'], resultado['resultados_busca_w'])\n",
    "    \n",
    "    logger.info('Dados salvos\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
